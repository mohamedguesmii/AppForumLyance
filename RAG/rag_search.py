import json
import os
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import openai
from dotenv import load_dotenv

# Charger les variables d'environnement depuis rag.env
load_dotenv("rag.env")
openai.api_key = os.getenv("OPENAI_API_KEY")

if not openai.api_key:
    raise ValueError("‚ùå Cl√© API OpenAI manquante. V√©rifie que 'OPENAI_API_KEY' est bien d√©fini dans rag.env.")

# Nom du fichier JSON contenant les donn√©es export√©es (offres + √©v√©nements)
DATA_JSON_FILE = 'data.json'

def charger_donnees(json_file):
    with open(json_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def preparer_documents(data):
    docs = []
    for item in data:
        if item["type"] == "offre":
            txt = (f"Offre: {item.get('titre','')}. Description: {item.get('description','')}. "
                   f"Lieu: {item.get('lieu','')}. Date: {item.get('date_publication','')}. "
                   f"Type: {item.get('offre_type','')}. Domaine: {item.get('domaine','')}")
        else:  # evenement
            txt = (f"√âv√©nement: {item.get('titre','')}. Description: {item.get('description','')}. "
                   f"Lieu: {item.get('lieu','')}. Dates: {item.get('date_debut','')} au {item.get('date_fin','')}")
        docs.append(txt)
    return docs

def creer_index_faiss(embeddings):
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(embeddings))
    return index

def rechercher(question, model, index, documents, k=3):
    question_embedding = model.encode([question])
    distances, indices = index.search(np.array(question_embedding), k)
    return [documents[idx] for idx in indices[0]]

def generer_reponse(question, documents):
    context = "\n\n".join(documents)
    messages = [
        {"role": "system", "content": "Tu es un assistant utile qui r√©pond de mani√®re claire et concise."},
        {"role": "user", "content": f"Voici des informations extraites de la base :\n{context}\n\nQuestion : {question}\nR√©ponse courte et pr√©cise :"}
    ]
    
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=150,
        temperature=0.3
    )
    return response.choices[0].message.content.strip()

def main():
    print("Chargement des donn√©es...")
    data = charger_donnees(DATA_JSON_FILE)

    print("Pr√©paration des documents...")
    documents = preparer_documents(data)

    print("Chargement du mod√®le d'embeddings...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    print("Cr√©ation des embeddings...")
    embeddings = model.encode(documents)

    print("Cr√©ation de l'index FAISS...")
    index = creer_index_faiss(embeddings)

    print("‚úÖ Syst√®me RAG pr√™t. Pose ta question (tape 'exit' pour quitter).")

    while True:
        question = input("\n‚ùì Question : ")
        if question.lower() == 'exit':
            print("üëã Fin du programme.")
            break

        print("\nüîç Recherche des documents pertinents...")
        docs_trouves = rechercher(question, model, index, documents)

        print("\nüìÑ Documents pertinents trouv√©s :")
        for d in docs_trouves:
            print("-", d)

        print("\nü§ñ G√©n√©ration de la r√©ponse...")
        reponse = generer_reponse(question, docs_trouves)

        print("\n‚úÖ R√©ponse :")
        print(reponse)

if __name__ == "__main__":
    main()
